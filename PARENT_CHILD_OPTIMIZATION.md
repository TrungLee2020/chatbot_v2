# PARENT-CHILD RETRIEVAL OPTIMIZATION

> **Context**: B·∫°n ƒëang d√πng parent-child retrieval pattern ƒë√∫ng ƒë·∫Øn
>
> **Fixed**: Chunk size = 1024 tokens (embedding time)
>
> **Flexible**: Window size & context merging (retrieval time)

---

## üéØ APPROACH HI·ªÜN T·∫†I (T·ªët!)

### **Indexing Phase:**
```python
# Small, fixed chunks ‚Üí ch√≠nh x√°c cho embedding
chunk_size = 1024
chunk_overlap = 256

# Store parent context in metadata
for i, chunk in enumerate(chunks):
    metadata = {
        'text': chunk,                          # Main chunk - EMBED THIS
        'context_before_text': chunks[i-2:i],   # 2 chunks before
        'context_after_text': chunks[i+1:i+3],  # 2 chunks after
        'chunk_index': i
    }
```

### **Retrieval Phase:**
```python
# 1. Retrieve small chunk (precise match)
retrieved_chunk = vector_search(query)  # 1024 tokens

# 2. Merge with parent context
full_context = merge_with_context(
    chunk=retrieved_chunk,
    max_tokens=1024,
    before_weight=0.4,  # 40% cho before
    after_weight=0.6    # 60% cho after
)

# 3. Send to LLM
llm.generate(context=full_context, query=query)
```

**Benefits:**
- ‚úÖ Embeddings consistent (all chunks ~1024 tokens)
- ‚úÖ Retrieval precise (small semantic units)
- ‚úÖ Context flexible (adjust at query time)
- ‚úÖ No re-indexing needed

---

## üìä V·∫§N ƒê·ªÄ C·∫¶N C·∫¢I THI·ªÜN

### ‚ùå **Problem 1: Fixed Window Size**

```python
# 1_document_splitting.py:222
chunks_data = custom_split(content, window_size=2)  # ‚ùå Always 2

# query_server_new.py:261
# Window size = 1 (1 chunk tr∆∞·ªõc + chunk hi·ªán t·∫°i + 1 chunk sau)
```

**Issues:**
- T√†i li·ªáu "Q&A ng·∫Øn" (dvkh) ‚Üí window=2 qu√° l·ªõn, th√™m noise
- T√†i li·ªáu "Quy ƒë·ªãnh d√†i" (ktcn) ‚Üí window=2 qu√° nh·ªè, thi·∫øu context
- Kh√¥ng adapt theo query type

---

### ‚ùå **Problem 2: Fixed Token Allocation**

```python
# query_server_new.py:306-308
max_before_tokens = int(available_tokens * 0.4)  # ‚ùå Always 40/60
max_after_tokens = available_tokens - max_before_tokens
```

**Issues:**
- C√¢u h·ªèi "ƒê·ªãnh nghƒ©a X l√† g√¨?" ‚Üí c·∫ßn nhi·ªÅu context AFTER (gi·∫£i th√≠ch)
- C√¢u h·ªèi "Nguy√™n nh√¢n c·ªßa X?" ‚Üí c·∫ßn nhi·ªÅu context BEFORE (background)
- Fixed 40/60 kh√¥ng optimal cho m·ªçi tr∆∞·ªùng h·ª£p

---

### ‚ùå **Problem 3: Binary Context Inclusion**

```python
# query_server_new.py:315-333
if context_before and max_before_tokens > 50:
    parts.append(f"[Ng·ªØ c·∫£nh tr∆∞·ªõc]\n{context_before}\n")  # ‚ùå All or nothing
```

**Issues:**
- Kh√¥ng rank chunks in context window
- C√≥ th·ªÉ chunk_before[0] irrelevant nh∆∞ng v·∫´n add
- Waste tokens on low-quality context

---

## üí° ƒê·ªÄ XU·∫§T C·∫¢I THI·ªÜN (Retrieval-Time Optimization)

### ‚úÖ **Improvement 1: Adaptive Window Size theo Topic & Query**

<details>
<summary><b>üìå Implementation: Smart Context Window</b></summary>

```python
# app/services/context_merging_service.py
from typing import Dict, List, Optional
from app.core.constants import VALID_TOPICS
from app.utils.text_processing import get_token_length

class SmartContextMerger:
    """
    Adaptive context window based on:
    1. Topic characteristics
    2. Query type
    3. Chunk position in document
    """

    # Topic-specific window configurations
    TOPIC_WINDOW_CONFIG = {
        # Q&A topics ‚Üí smaller windows (self-contained)
        "dvkh": {"default_window": 1, "max_window": 2},
        "hcc": {"default_window": 1, "max_window": 2},

        # Technical/procedural ‚Üí medium windows
        "ktcn": {"default_window": 2, "max_window": 3},
        "qlcl": {"default_window": 2, "max_window": 3},
        "bccp_nd": {"default_window": 2, "max_window": 3},

        # Financial/complex ‚Üí larger windows (interconnected)
        "tcbc": {"default_window": 3, "max_window": 4},
        "tcns": {"default_window": 3, "max_window": 4},

        # Default
        "default": {"default_window": 2, "max_window": 3}
    }

    def __init__(self):
        self.query_classifier = QueryClassifier()

    def determine_window_size(
        self,
        topic: Optional[str],
        query: str,
        chunk_metadata: Dict
    ) -> Dict[str, int]:
        """
        Determine optimal window size.

        Returns:
            {"before": N, "after": M}
        """
        # Get base config for topic
        config = self.TOPIC_WINDOW_CONFIG.get(
            topic,
            self.TOPIC_WINDOW_CONFIG["default"]
        )

        default_window = config["default_window"]
        max_window = config["max_window"]

        # Classify query type
        query_type = self.query_classifier.classify(query)

        # Adjust based on query type
        if query_type == QueryType.DEFINITION:
            # Definitions need more AFTER context (explanation)
            return {
                "before": max(1, default_window - 1),
                "after": min(max_window, default_window + 1)
            }

        elif query_type == QueryType.PROCEDURAL:
            # Procedures need sequential context
            return {
                "before": default_window,
                "after": default_window
            }

        elif query_type == QueryType.FACTUAL:
            # Facts often self-contained
            return {
                "before": max(1, default_window - 1),
                "after": max(1, default_window - 1)
            }

        elif query_type == QueryType.COMPARISON:
            # Comparisons need broader context
            return {
                "before": max_window,
                "after": max_window
            }

        # Check chunk position
        chunk_index = chunk_metadata.get('chunk_index', -1)
        total_chunks = chunk_metadata.get('total_chunks', 0)

        # First chunk ‚Üí reduce before window
        if chunk_index == 0:
            return {
                "before": 0,
                "after": default_window + 1
            }

        # Last chunk ‚Üí reduce after window
        if chunk_index == total_chunks - 1:
            return {
                "before": default_window + 1,
                "after": 0
            }

        # Default: symmetric
        return {
            "before": default_window,
            "after": default_window
        }

    def merge_with_adaptive_window(
        self,
        main_chunk: str,
        context_before_chunks: List[str],
        context_after_chunks: List[str],
        query: str,
        topic: Optional[str],
        max_tokens: int = 1024
    ) -> Dict:
        """
        Merge chunk with adaptive context window.

        Returns:
            {
                "text": merged_text,
                "stats": {...}
            }
        """
        chunk_metadata = {
            'chunk_index': len(context_before_chunks),
            'total_chunks': len(context_before_chunks) + 1 + len(context_after_chunks)
        }

        # Determine window size
        window = self.determine_window_size(topic, query, chunk_metadata)

        # Calculate tokens
        main_tokens = get_token_length(main_chunk)
        if main_tokens >= max_tokens:
            return {
                "text": main_chunk,
                "stats": {
                    "window_before": 0,
                    "window_after": 0,
                    "truncated": True
                }
            }

        available_tokens = max_tokens - main_tokens

        # Get context based on adaptive window
        selected_before = context_before_chunks[-window["before"]:] if window["before"] > 0 else []
        selected_after = context_after_chunks[:window["after"]] if window["after"] > 0 else []

        # Allocate tokens (still adaptive!)
        allocation = self._get_token_allocation(query)
        max_before_tokens = int(available_tokens * allocation["before"])
        max_after_tokens = int(available_tokens * allocation["after"])

        # Build context
        parts = []
        actual_before_tokens = 0
        actual_after_tokens = 0

        # Add before context
        if selected_before:
            before_text = "\n===CHUNK_SEP===\n".join(selected_before)
            before_tokens = get_token_length(before_text)

            if before_tokens <= max_before_tokens:
                parts.append(f"[Ng·ªØ c·∫£nh tr∆∞·ªõc]\n{before_text}\n")
                actual_before_tokens = before_tokens
            else:
                # Truncate from beginning (keep most recent)
                ratio = max_before_tokens / before_tokens
                char_limit = int(len(before_text) * ratio)
                truncated = "..." + before_text[-char_limit:]
                parts.append(f"[Ng·ªØ c·∫£nh tr∆∞·ªõc]\n{truncated}\n")
                actual_before_tokens = max_before_tokens

        # Add main chunk
        parts.append(f"[N·ªôi dung ch√≠nh]\n{main_chunk}\n")

        # Add after context
        if selected_after:
            after_text = "\n===CHUNK_SEP===\n".join(selected_after)
            after_tokens = get_token_length(after_text)

            if after_tokens <= max_after_tokens:
                parts.append(f"[Ng·ªØ c·∫£nh sau]\n{after_text}")
                actual_after_tokens = after_tokens
            else:
                # Truncate from end (keep most immediate)
                ratio = max_after_tokens / after_tokens
                char_limit = int(len(after_text) * ratio)
                truncated = after_text[:char_limit] + "..."
                parts.append(f"[Ng·ªØ c·∫£nh sau]\n{truncated}")
                actual_after_tokens = max_after_tokens

        merged_text = "\n".join(parts)

        return {
            "text": merged_text,
            "stats": {
                "window_before": len(selected_before),
                "window_after": len(selected_after),
                "tokens_before": actual_before_tokens,
                "tokens_after": actual_after_tokens,
                "tokens_main": main_tokens,
                "tokens_total": get_token_length(merged_text)
            }
        }

    def _get_token_allocation(self, query: str) -> Dict[str, float]:
        """
        Determine token allocation ratio based on query.

        Returns:
            {"before": 0.4, "after": 0.6}
        """
        query_type = self.query_classifier.classify(query)

        if query_type == QueryType.DEFINITION:
            # Definitions: more after (explanation follows term)
            return {"before": 0.3, "after": 0.7}

        elif query_type == QueryType.PROCEDURAL:
            # Procedures: balanced (step-by-step)
            return {"before": 0.5, "after": 0.5}

        elif query_type == QueryType.FACTUAL:
            # Facts: slightly more after
            return {"before": 0.4, "after": 0.6}

        else:
            # Default: current allocation
            return {"before": 0.4, "after": 0.6}
```

**Usage:**
```python
# In RAG service
context_merger = SmartContextMerger()

for result in retrieved_chunks:
    merged = context_merger.merge_with_adaptive_window(
        main_chunk=result.node.text,
        context_before_chunks=result.metadata['context_before_text'],
        context_after_chunks=result.metadata['context_after_text'],
        query=user_query,
        topic=topic_filter,
        max_tokens=1024
    )

    contexts.append(merged["text"])
```

**Expected Impact:**
- üéØ Context relevance: +20-25%
- üéØ Token efficiency: +15%
- üéØ Precision: +10% (less noise)

</details>

---

### ‚úÖ **Improvement 2: Selective Context Ranking**

<details>
<summary><b>üìå Rank & Filter Context Chunks</b></summary>

**Problem:** Hi·ªán t·∫°i, t·∫•t c·∫£ chunks trong window ƒë·ªÅu ƒë∆∞·ª£c add ‚Üí c√≥ th·ªÉ c√≥ noise.

**Solution:** Rank context chunks v√† ch·ªâ l·∫•y nh·ªØng chunks relevant nh·∫•t.

```python
# app/services/context_ranking_service.py
from typing import List, Tuple
import numpy as np

class ContextRankingService:
    """
    Rank context chunks by relevance to query.
    Only include high-quality context.
    """

    def __init__(self, reranker):
        self.reranker = reranker

    def rank_context_chunks(
        self,
        query: str,
        main_chunk: str,
        context_before: List[str],
        context_after: List[str],
        threshold: float = 0.2
    ) -> Tuple[List[str], List[str]]:
        """
        Rank and filter context chunks.

        Returns:
            (filtered_before, filtered_after)
        """
        # Rank before chunks
        if context_before:
            before_scores = self._score_chunks(query, context_before)
            # Keep only above threshold, maintain order
            filtered_before = [
                chunk for chunk, score in zip(context_before, before_scores)
                if score >= threshold
            ]
        else:
            filtered_before = []

        # Rank after chunks
        if context_after:
            after_scores = self._score_chunks(query, context_after)
            filtered_after = [
                chunk for chunk, score in zip(context_after, after_scores)
                if score >= threshold
            ]
        else:
            filtered_after = []

        return filtered_before, filtered_after

    def _score_chunks(
        self,
        query: str,
        chunks: List[str]
    ) -> List[float]:
        """
        Score chunks by relevance to query.
        Use lightweight reranker for speed.
        """
        if not chunks:
            return []

        # Use reranker
        results = self.reranker.rerank(
            corpus=chunks,
            query=query
        )

        # Extract scores
        scores = [0.0] * len(chunks)
        for result in results["results"]:
            idx = result["index"]
            scores[idx] = result["relevance_score"]

        return scores
```

**Integration:**
```python
context_ranker = ContextRankingService(lightweight_reranker)

# Before merging
filtered_before, filtered_after = context_ranker.rank_context_chunks(
    query=user_query,
    main_chunk=main_chunk,
    context_before=context_before_chunks,
    context_after=context_after_chunks,
    threshold=0.2  # Adaptive threshold
)

# Then merge with filtered chunks
merged = context_merger.merge_with_adaptive_window(
    main_chunk=main_chunk,
    context_before_chunks=filtered_before,  # Filtered!
    context_after_chunks=filtered_after,    # Filtered!
    ...
)
```

**Expected Impact:**
- üéØ Noise reduction: -30%
- üéØ Precision: +12%
- üéØ Token efficiency: +20% (ch·ªâ gi·ªØ relevant context)

</details>

---

### ‚úÖ **Improvement 3: Dynamic Context Budget**

<details>
<summary><b>üìå Adjust max_tokens based on query complexity</b></summary>

```python
# app/services/context_budget_service.py

class ContextBudgetService:
    """Determine optimal context size based on query complexity."""

    def calculate_context_budget(
        self,
        query: str,
        query_type: QueryType,
        topic: Optional[str]
    ) -> int:
        """
        Calculate optimal max_tokens for context.

        Simple queries ‚Üí smaller context
        Complex queries ‚Üí larger context
        """
        # Base budget
        base_budget = 1024

        # Adjust by query type
        if query_type == QueryType.FACTUAL:
            # Simple facts ‚Üí smaller budget
            budget = int(base_budget * 0.7)  # 716 tokens

        elif query_type == QueryType.DEFINITION:
            # Definitions ‚Üí medium budget
            budget = base_budget  # 1024 tokens

        elif query_type in [QueryType.PROCEDURAL, QueryType.COMPARISON]:
            # Complex ‚Üí larger budget
            budget = int(base_budget * 1.3)  # 1331 tokens

        elif query_type == QueryType.COMPLEX:
            # Very complex ‚Üí max budget
            budget = int(base_budget * 1.5)  # 1536 tokens

        else:
            budget = base_budget

        # Adjust by query length (longer query = more complex)
        query_tokens = get_token_length(query)
        if query_tokens > 50:
            budget = int(budget * 1.2)
        elif query_tokens < 15:
            budget = int(budget * 0.9)

        # Topic-specific adjustments
        if topic in ["tcbc", "tcns", "ktcn"]:
            # Complex topics ‚Üí more context
            budget = int(budget * 1.1)

        return min(budget, 2048)  # Hard cap at 2048
```

**Usage:**
```python
budget_service = ContextBudgetService()

# Calculate dynamic budget
max_tokens = budget_service.calculate_context_budget(
    query=user_query,
    query_type=detected_query_type,
    topic=topic_filter
)

# Use in merging
merged = context_merger.merge_with_adaptive_window(
    ...,
    max_tokens=max_tokens  # Dynamic!
)
```

**Expected Impact:**
- üéØ Token efficiency: +25%
- üéØ Response quality: +8%
- üéØ Latency: -10% (smaller contexts for simple queries)

</details>

---

## üìä C·∫¢I TI·∫æN CHUNK SIZE (Optional)

**C√≥ n√™n thay ƒë·ªïi chunk_size khi indexing?**

### **Current: 1024 tokens**

**Pros:**
- ‚úÖ Proven to work
- ‚úÖ Good balance
- ‚úÖ Fits trong context window

**Cons:**
- ‚ö†Ô∏è C√≥ th·ªÉ kh√¥ng optimal cho m·ªçi content type

### **Testing Recommendation:**

Th·ª≠ A/B test v·ªõi **3 chunk sizes** tr√™n sample data:

| Chunk Size | Best For | Expected Precision |
|------------|----------|-------------------|
| **768** | Q&A, short documents (dvkh, hcc) | +5% for short queries |
| **1024** | General purpose (current) | Baseline |
| **1280** | Complex documents (ktcn, tcbc, qlcl) | +3% for complex queries |

**How to test:**
```python
# Create 3 test collections
collections = {
    "test_768": index_with_chunk_size(768),
    "test_1024": index_with_chunk_size(1024),  # Current
    "test_1280": index_with_chunk_size(1280)
}

# Run test queries
for query, ground_truth in test_set:
    for size, collection in collections.items():
        results = retrieve(query, collection)
        precision[size] = evaluate(results, ground_truth)

# Compare
print(f"768: {precision['768']:.2%}")
print(f"1024: {precision['1024']:.2%}")  # Baseline
print(f"1280: {precision['1280']:.2%}")
```

**My prediction:**
- 1024 v·∫´n t·ªët nh·∫•t cho **majority** c·ªßa documents
- Ch·ªâ c·∫£i thi·ªán marginal (~2-3%) n·∫øu thay ƒë·ªïi

**Recommendation:** Gi·ªØ 1024, focus v√†o optimize retrieval-time context merging (impact l·ªõn h∆°n nhi·ªÅu!)

---

## üéØ IMPLEMENTATION PRIORITY

### **Phase 1: High Impact, Low Effort** (1 tu·∫ßn)

1. ‚úÖ **Adaptive Window Size**
   - Create `SmartContextMerger` service
   - Define `TOPIC_WINDOW_CONFIG`
   - Update `get_chunk_with_context()`
   - **Expected: +20% context relevance**

2. ‚úÖ **Dynamic Token Allocation**
   - Add `_get_token_allocation()` logic
   - Query-type aware allocation
   - **Expected: +15% token efficiency**

### **Phase 2: Medium Impact** (1 tu·∫ßn)

3. ‚úÖ **Selective Context Ranking**
   - Create `ContextRankingService`
   - Integrate lightweight reranker
   - **Expected: +12% precision, -30% noise**

4. ‚úÖ **Dynamic Context Budget**
   - Create `ContextBudgetService`
   - Query-complexity aware budgets
   - **Expected: +8% quality, -10% latency**

### **Phase 3: Optional** (n·∫øu c·∫ßn)

5. ‚ö†Ô∏è **A/B Test Chunk Sizes**
   - Test 768 vs 1024 vs 1280
   - Measure on your actual queries
   - **Expected: +2-3% marginal gain**

---

## üìà EXPECTED OVERALL IMPACT

| Metric | Before | After Phase 1 | After Phase 2 |
|--------|--------|---------------|---------------|
| **Context Relevance** | Baseline | +20% | +25% |
| **Token Efficiency** | Baseline | +15% | +25% |
| **Precision** | Baseline | +5% | +12% |
| **Query Latency** | Baseline | No change | -10% |

**Total Expected Gain: +20-30% improvement** without re-indexing!

---

## ‚úÖ T√ìM T·∫ÆT

### **B·∫°n ƒë√£ ƒë√∫ng:**
- ‚úÖ Parent-child retrieval l√† approach t·ªët
- ‚úÖ Fixed small chunks ‚Üí precise embeddings
- ‚úÖ Flexible context at retrieval time

### **C·∫ßn c·∫£i thi·ªán (retrieval-time):**
1. Adaptive window size (fixed ‚Üí dynamic)
2. Smart token allocation (40/60 ‚Üí query-aware)
3. Context chunk ranking (all ‚Üí selective)
4. Dynamic context budget (1024 ‚Üí adaptive)

### **Kh√¥ng n√™n:**
- ‚ùå Thay ƒë·ªïi chunk_size khi indexing (tr·ª´ khi A/B test prove it)
- ‚ùå Re-index to√†n b·ªô
- ‚ùå L√†m ph·ª©c t·∫°p indexing pipeline

### **N√™n:**
- ‚úÖ Focus v√†o optimize retrieval-time logic
- ‚úÖ Implement t·ª´ng improvement m·ªôt
- ‚úÖ Measure impact sau m·ªói change
- ‚úÖ Keep indexing simple, make retrieval smart

---

B·∫°n mu·ªën t√¥i t·∫°o code implementation cho improvements n√†y kh√¥ng? üöÄ
